{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "dataset_dir = r'C:\\Users\\fardin\\Projects\\EnhanceSEO\\datasets\\extractedURLs\\url_classes.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from transformers import BertTokenizer\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_dir)\n",
    "class_list = df.Topic.unique()\n",
    "class_id = {t:i for i, t in enumerate(class_list)}\n",
    "id_class = {i:t for i, t in enumerate(class_list)}\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1024\n",
    "max_token_length = 64\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = {t: i for i, t in enumerate(bert_tokenizer.vocab)}\n",
    "vocab_size = len(bert_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, num_classes, dictionary, tokenizer, doc_length=256) -> None:\n",
    "        super().__init__()\n",
    "        self.doc_length = doc_length\n",
    "        y = torch.from_numpy(np.array([class_id[c] for c in y], dtype=np.longlong))\n",
    "        self.y = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        self.dictionary = dictionary\n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(self.dictionary)\n",
    "        \n",
    "        self.X = torch.zeros((len(X), doc_length), dtype=torch.int)\n",
    "        for i, doc in enumerate(X):\n",
    "            indices = torch.from_numpy(np.array(self.tokenizer(doc)['input_ids'], dtype=np.longlong))\n",
    "            pad_size = max(self.doc_length - len(indices), 0)\n",
    "            self.X[i] = torch.nn.functional.pad(indices[:self.doc_length], (0,pad_size))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "train_dataset = CustomDataset(df_train.Address.values, df_train.Topic.values, len(class_id), vocab_dict, bert_tokenizer, doc_length=max_token_length)\n",
    "test_dataset = CustomDataset(df_test.Address.values, df_test.Topic.values, len(class_id), vocab_dict, bert_tokenizer, doc_length=max_token_length)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])\n",
      "[101, 14120, 131, 120, 120, 10494, 119, 46484, 19094, 119, 10212, 120, 41163, 118, 11131, 120, 10751, 118, 32784, 118, 43045, 120, 72812, 10681, 118, 14609, 118, 10635, 12953, 118, 10155, 118, 16118, 102]\n",
      "[101, 14120, 131, 120, 120, 17045, 14752, 57192, 119, 41181, 119, 10212, 120, 13617, 120, 68257, 69486, 10133, 118, 10143, 118, 38973, 10598, 118, 11303, 118, 187, 118, 66626, 10107, 120, 143, 10874, 12022, 10237, 11779, 10246, 10884, 10418, 19282, 36237, 10874, 136, 53264, 134, 10110, 102]\n"
     ]
    }
   ],
   "source": [
    "for val in df_train.Address.values[:1]:\n",
    "    print(train_dataset.tokenizer(val).keys())\n",
    "for val in df_train.Address.values[:2]:\n",
    "    print(train_dataset.tokenizer(val)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset time = 0.0240631103515625\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "X, y = next(iter(test_dataloader))\n",
    "print(f'dataset time = {time.time() - st}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_for_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, max_token_count, embedding_dim=64, dropout=0.3, num_out_features=4, *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text, self).__init__(*args, **kwargs)\n",
    "        self.max_token_count = max_token_count\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, 64, kernel_size=5, padding=2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(64, 64, kernel_size=3, padding=1)\n",
    "        self.globalpool = nn.AdaptiveMaxPool1d(32)\n",
    "        self.fc1 = nn.Linear(64 * max_token_count//2, 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, num_out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.globalpool(x)\n",
    "        x = F.relu(self.fc1(x.view(x.shape[0], -1)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_for_Text(num_embedding=vocab_size, max_token_count=max_token_length, embedding_dim=96, num_out_features=len(class_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1024, 12])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "import lightning as L\n",
    "# from abc import abstractmethod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(ClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(\"model\", logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x)\n",
    "\n",
    "    def on_train_epoch_start(self) -> None:\n",
    "        param_groups = next(iter(self.optimizer.param_groups))\n",
    "        if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "            current_learning_rate = float(param_groups[\"lr\"])\n",
    "            self.log(\n",
    "                \"lr\",\n",
    "                current_learning_rate,\n",
    "                batch_size=self.batch_size,\n",
    "                on_epoch=True,\n",
    "                on_step=False,\n",
    "            )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lr=  0.001380384264602885\n",
    "# 0.00010631317724117211\n",
    "output_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "119547"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bert_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 64\n",
    "embedding_dim = 64\n",
    "label_size = 1\n",
    "\n",
    "classifier_torch_model = CNN_for_Text(num_embedding=vocab_size, max_token_count=max_token_length, num_out_features=len(class_id)).to(device)\n",
    "optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=0.00012)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[30, 60, 90, 120],gamma=0.5)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "classfier_lightning_model = ClassifierLightningModel(classifier_torch_model, \n",
    "                                                     num_classes=len(class_id),\n",
    "                                            learning_rate=lr,\n",
    "                                            batch_size=batch_size,\n",
    "                                            optimizer=optimizer,\n",
    "                                            loss_func=loss_func,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            user_lr_scheduler=True\n",
    "                                            ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import lightning as L\n",
    "\n",
    "# from scripts.utils.CustomCallbacks.ModelCheckpoint import CustomModelCheckpoint\n",
    "\n",
    "# callbacks = [\n",
    "#         CustomModelCheckpoint(dirpath=r'models\\model2_word_embedding-256-2', filename='str_embedding', every_n_epochs=1, mode='min', monitor='train_loss', save_on_train_epoch_end=True),\n",
    "#         ModelCheckpoint(save_top_k=5, mode='min', monitor='train_loss', save_last=True)\n",
    "#         ]\n",
    "trainer = L.Trainer(\n",
    "            # callbacks=callbacks,\n",
    "            max_epochs=400,\n",
    "            accelerator= 'gpu' if device==torch.device(\"cuda\") else 'cpu',\n",
    "            logger=CSVLogger(save_dir='logs/', name='log2'), \n",
    "            num_sanity_val_steps=0,\n",
    "        #     default_root_dir='models\\model2_word_embedding-256-2'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.tuner import Tuner\n",
    "# tuner = Tuner(trainer)\n",
    "# tuning_result = tuner.lr_find(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader, min_lr=0.00001,max_lr=0.01, num_training=100)\n",
    "\n",
    "# fig = tuning_result.plot(suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# max_epochs = 1000\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# trainer.fit_loop.max_epochs = max_epochs\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mfit(classfier_lightning_model, train_dataloaders\u001b[38;5;241m=\u001b[39mtrain_dataloader, val_dataloaders\u001b[38;5;241m=\u001b[39mtest_dataloader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# max_epochs = 1000\n",
    "# trainer.fit_loop.max_epochs = max_epochs\n",
    "trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from torchmetrics import ConfusionMatrix\n",
    "\n",
    "def calculate_metrics(cl_model):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(test_dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 23.26it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6679    0.7469    0.7052       245\n",
      "           1     0.8045    0.7985    0.8015       268\n",
      "           2     0.9218    0.8421    0.8802       266\n",
      "           3     0.7888    0.7529    0.7704       263\n",
      "           4     0.5966    0.7184    0.6519       245\n",
      "           5     0.6884    0.6227    0.6539       220\n",
      "           6     0.7925    0.7095    0.7487       296\n",
      "           7     0.8161    0.8486    0.8320       251\n",
      "           8     0.6830    0.6990    0.6909       299\n",
      "           9     0.6382    0.6875    0.6619       272\n",
      "          10     0.7787    0.7379    0.7578       248\n",
      "          11     0.8913    0.8241    0.8564       199\n",
      "\n",
      "    accuracy                         0.7480      3072\n",
      "   macro avg     0.7557    0.7490    0.7509      3072\n",
      "weighted avg     0.7543    0.7480    0.7498      3072\n",
      "\n",
      "confusion matrix:\n",
      " tensor([[183,   5,   5,   2,   6,   1,   7,   1,  29,   4,   1,   1],\n",
      "        [  9, 214,   1,   2,  10,   9,  10,   4,   4,   2,   0,   3],\n",
      "        [  1,   1, 224,   6,   6,   2,   0,   4,   3,   9,   9,   1],\n",
      "        [  4,   1,   0, 198,  12,   6,   2,  11,   6,  16,   6,   1],\n",
      "        [  8,   8,   1,   5, 176,   5,   2,   4,  12,  15,   3,   6],\n",
      "        [  3,  14,   0,   6,  11, 137,  10,   8,   5,  12,  12,   2],\n",
      "        [  9,  10,   1,   2,  11,  16, 210,   4,   4,  22,   6,   1],\n",
      "        [  4,   1,   2,   9,   9,   5,   1, 213,   2,   1,   4,   0],\n",
      "        [ 43,   6,   2,   3,  10,   4,   3,   2, 209,  14,   2,   1],\n",
      "        [  5,   1,   1,  14,  19,   7,  11,   1,  19, 187,   6,   1],\n",
      "        [  3,   4,   5,   4,  13,   6,   5,   8,   6,   8, 183,   3],\n",
      "        [  2,   1,   1,   0,  12,   1,   4,   1,   7,   3,   3, 164]])\n",
      "================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "classfier_lightning_model = classfier_lightning_model.eval()\n",
    "calculate_metrics(classfier_lightning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S': 83,\n",
       " 'h': 104,\n",
       " 'o': 111,\n",
       " 'r': 114,\n",
       " 't': 116,\n",
       " ' ': 32,\n",
       " 's': 115,\n",
       " 'y': 121,\n",
       " '.': 46,\n",
       " '~': 126}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = \"Short story.~\"\n",
    "{c:ord(c) for c in my_str}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
