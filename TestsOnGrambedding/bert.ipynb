{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "train_dir = r'datasets\\Grambedding\\test.csv'\n",
    "test_dir = r'datasets\\Grambedding\\train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# !pip install transformers datasets torch evaluate\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from calflops import calculate_flops\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000])\n",
      "tensor(1.0162)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_nums = torch.randn(5000)\n",
    "print(rand_nums.shape)\n",
    "print(torch.std(rand_nums))\n",
    "plt.hist(rand_nums, bins=500)\n",
    "plt.hist(torch.fmod(rand_nums,2), bins=250)\n",
    "plt.hist(torch.fmod(rand_nums,2.)*(2./3.), bins=250)\n",
    "plt.xlim([-4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fardin\\AppData\\Local\\Temp\\ipykernel_23756\\3619570887.py:1: ParserWarning: Skipping line 7: expected 2 fields, saw 3\n",
      "Skipping line 69227: expected 2 fields, saw 3\n",
      "Skipping line 82729: expected 2 fields, saw 3\n",
      "Skipping line 161388: expected 2 fields, saw 3\n",
      "Skipping line 293847: expected 2 fields, saw 3\n",
      "Skipping line 334928: expected 2 fields, saw 3\n",
      "Skipping line 456118: expected 2 fields, saw 3\n",
      "\n",
      "  df_train = pd.read_csv(r'datasets\\Grambedding\\train.csv', header=None, on_bad_lines='warn', encoding = 'ISO-8859-1', low_memory=False)\n",
      "C:\\Users\\fardin\\AppData\\Local\\Temp\\ipykernel_23756\\3619570887.py:5: ParserWarning: Skipping line 9533: expected 2 fields, saw 3\n",
      "Skipping line 158818: expected 2 fields, saw 3\n",
      "\n",
      "  df_test = pd.read_csv(r'datasets\\Grambedding\\test.csv', header=None, on_bad_lines='warn', encoding = 'ISO-8859-1', low_memory=False)\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(r'datasets\\Grambedding\\train.csv', header=None, on_bad_lines='warn', encoding = 'ISO-8859-1', low_memory=False)\n",
    "df_train.dropna(inplace=True)\n",
    "df_train.columns = ['label', 'text']\n",
    "df_train.label -= 1\n",
    "df_test = pd.read_csv(r'datasets\\Grambedding\\test.csv', header=None, on_bad_lines='warn', encoding = 'ISO-8859-1', low_memory=False)\n",
    "df_test.dropna(inplace=True)\n",
    "df_test.columns = ['label', 'text']\n",
    "df_test.label -= 1\n",
    "\n",
    "df = pd.concat([df_train, df_test], axis=0)\n",
    "df.dropna(inplace=True)\n",
    "class_list = df.label.unique()\n",
    "class_id = {'benign':1, 'malicious': 0}\n",
    "id_class = {1: 'benign', 0:'malicious'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'text'], dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(dataset_dir, on_bad_lines='warn', encoding = 'ISO-8859-1', low_memory=False)[['domain','label']]\n",
    "# df.columns=['text', 'Topic']\n",
    "# df.dropna(inplace=True)\n",
    "# class_list = df.Topic.unique()\n",
    "# class_id = {'benign':0, 'phishing': 1}\n",
    "# id_class = {0: 'benign', 1:'phishing'}\n",
    "# df['label'] = np.array([t for t in df['Topic']], dtype=int)\n",
    "# df = df.drop('Topic', axis=1)\n",
    "# df_train, df_test = train_test_split(df, test_size=0.1, shuffle=True)\n",
    "# print(df_train.label.value_counts())\n",
    "# print(df_test.label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3d513e672124767ad7e286c98d1d8cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/639986 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e97d7e5ab9b4e1cba53a392c9430498",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/159996 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=64, padding='max_length')\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format for PyTorch\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "test_dataset = test_dataset.rename_column('label', 'labels')\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=r'logs/OtherModels/bert_ag_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Load the metrics\n",
    "import evaluate;\n",
    "accuracy_metric = evaluate.load('accuracy', trust_remote_code=True)\n",
    "precision_metric = evaluate.load('precision', trust_remote_code=True)\n",
    "recall_metric = evaluate.load('recall', trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p.predictions, p.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall']\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1570"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38781b3214ab478c996b4b5103e22f46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1548, 'learning_rate': 1.9600000000000002e-05, 'epoch': 0.2}\n",
      "{'loss': 0.0824, 'learning_rate': 1.9200000000000003e-05, 'epoch': 0.4}\n",
      "{'loss': 0.0724, 'learning_rate': 1.88e-05, 'epoch': 0.6}\n",
      "{'loss': 0.0638, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.8}\n",
      "{'loss': 0.0595, 'learning_rate': 1.8e-05, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd6316f033d4edf8088f2bb4936ee1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.056943971663713455, 'eval_accuracy': 0.97931198279957, 'eval_precision': 0.9796275563389851, 'eval_recall': 0.97931198279957, 'eval_runtime': 170.4527, 'eval_samples_per_second': 938.653, 'eval_steps_per_second': 3.667, 'epoch': 1.0}\n",
      "{'loss': 0.0466, 'learning_rate': 1.76e-05, 'epoch': 1.2}\n",
      "{'loss': 0.046, 'learning_rate': 1.72e-05, 'epoch': 1.4}\n",
      "{'loss': 0.0447, 'learning_rate': 1.6800000000000002e-05, 'epoch': 1.6}\n",
      "{'loss': 0.0427, 'learning_rate': 1.64e-05, 'epoch': 1.8}\n",
      "{'loss': 0.0415, 'learning_rate': 1.6000000000000003e-05, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d3ab0ab9f3437a805262d96facbc39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.046376314014196396, 'eval_accuracy': 0.9836058401460036, 'eval_precision': 0.9836548145339405, 'eval_recall': 0.9836058401460036, 'eval_runtime': 170.695, 'eval_samples_per_second': 937.321, 'eval_steps_per_second': 3.662, 'epoch': 2.0}\n",
      "{'loss': 0.0293, 'learning_rate': 1.5600000000000003e-05, 'epoch': 2.2}\n",
      "{'loss': 0.0301, 'learning_rate': 1.5200000000000002e-05, 'epoch': 2.4}\n",
      "{'loss': 0.0292, 'learning_rate': 1.48e-05, 'epoch': 2.6}\n",
      "{'loss': 0.031, 'learning_rate': 1.4400000000000001e-05, 'epoch': 2.8}\n",
      "{'loss': 0.0307, 'learning_rate': 1.4e-05, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f49ec232eb47a9bb3ea86dd74a5a1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04816111549735069, 'eval_accuracy': 0.9845496137403436, 'eval_precision': 0.9845500062454735, 'eval_recall': 0.9845496137403436, 'eval_runtime': 170.3553, 'eval_samples_per_second': 939.19, 'eval_steps_per_second': 3.669, 'epoch': 3.0}\n",
      "{'loss': 0.0208, 'learning_rate': 1.3600000000000002e-05, 'epoch': 3.2}\n",
      "{'loss': 0.0204, 'learning_rate': 1.3200000000000002e-05, 'epoch': 3.4}\n",
      "{'loss': 0.0213, 'learning_rate': 1.2800000000000001e-05, 'epoch': 3.6}\n",
      "{'loss': 0.0208, 'learning_rate': 1.2400000000000002e-05, 'epoch': 3.8}\n",
      "{'loss': 0.021, 'learning_rate': 1.2e-05, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "256723911c39423588ddaeed574faa5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.0529664047062397, 'eval_accuracy': 0.9847058676466912, 'eval_precision': 0.9847058846879845, 'eval_recall': 0.9847058676466912, 'eval_runtime': 170.2044, 'eval_samples_per_second': 940.023, 'eval_steps_per_second': 3.672, 'epoch': 4.0}\n",
      "{'loss': 0.0142, 'learning_rate': 1.16e-05, 'epoch': 4.2}\n",
      "{'loss': 0.0135, 'learning_rate': 1.1200000000000001e-05, 'epoch': 4.4}\n",
      "{'loss': 0.0144, 'learning_rate': 1.0800000000000002e-05, 'epoch': 4.6}\n",
      "{'loss': 0.0138, 'learning_rate': 1.04e-05, 'epoch': 4.8}\n",
      "{'loss': 0.0148, 'learning_rate': 1e-05, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8051020d7bba4f90b5571070a2cd4582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.056854747235774994, 'eval_accuracy': 0.985587139678492, 'eval_precision': 0.9855947165488956, 'eval_recall': 0.985587139678492, 'eval_runtime': 162.8526, 'eval_samples_per_second': 982.459, 'eval_steps_per_second': 3.838, 'epoch': 5.0}\n",
      "{'loss': 0.0096, 'learning_rate': 9.600000000000001e-06, 'epoch': 5.2}\n",
      "{'loss': 0.0098, 'learning_rate': 9.200000000000002e-06, 'epoch': 5.4}\n",
      "{'loss': 0.0097, 'learning_rate': 8.8e-06, 'epoch': 5.6}\n",
      "{'loss': 0.0105, 'learning_rate': 8.400000000000001e-06, 'epoch': 5.8}\n",
      "{'loss': 0.0108, 'learning_rate': 8.000000000000001e-06, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5470e085fe30415da06bb019913a3c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.06325827538967133, 'eval_accuracy': 0.9855496387409686, 'eval_precision': 0.9855557582633843, 'eval_recall': 0.9855496387409686, 'eval_runtime': 163.3076, 'eval_samples_per_second': 979.722, 'eval_steps_per_second': 3.827, 'epoch': 6.0}\n",
      "{'loss': 0.0062, 'learning_rate': 7.600000000000001e-06, 'epoch': 6.2}\n",
      "{'loss': 0.0074, 'learning_rate': 7.2000000000000005e-06, 'epoch': 6.4}\n",
      "{'loss': 0.0073, 'learning_rate': 6.800000000000001e-06, 'epoch': 6.6}\n",
      "{'loss': 0.0068, 'learning_rate': 6.4000000000000006e-06, 'epoch': 6.8}\n",
      "{'loss': 0.0067, 'learning_rate': 6e-06, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97ad5aa5031f49b3b56ebbcb32c9f1f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07270927727222443, 'eval_accuracy': 0.9856308907722693, 'eval_precision': 0.9856329069325611, 'eval_recall': 0.9856308907722693, 'eval_runtime': 162.9797, 'eval_samples_per_second': 981.693, 'eval_steps_per_second': 3.835, 'epoch': 7.0}\n",
      "{'loss': 0.005, 'learning_rate': 5.600000000000001e-06, 'epoch': 7.2}\n",
      "{'loss': 0.0055, 'learning_rate': 5.2e-06, 'epoch': 7.4}\n",
      "{'loss': 0.0048, 'learning_rate': 4.800000000000001e-06, 'epoch': 7.6}\n",
      "{'loss': 0.0052, 'learning_rate': 4.4e-06, 'epoch': 7.8}\n",
      "{'loss': 0.0053, 'learning_rate': 4.000000000000001e-06, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a246bfecf0748d28b1b1fcc658cf49b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07957855612039566, 'eval_accuracy': 0.9851621290532263, 'eval_precision': 0.9851622385234298, 'eval_recall': 0.9851621290532264, 'eval_runtime': 162.8906, 'eval_samples_per_second': 982.23, 'eval_steps_per_second': 3.837, 'epoch': 8.0}\n",
      "{'loss': 0.004, 'learning_rate': 3.6000000000000003e-06, 'epoch': 8.2}\n",
      "{'loss': 0.0037, 'learning_rate': 3.2000000000000003e-06, 'epoch': 8.4}\n",
      "{'loss': 0.0038, 'learning_rate': 2.8000000000000003e-06, 'epoch': 8.6}\n",
      "{'loss': 0.0039, 'learning_rate': 2.4000000000000003e-06, 'epoch': 8.8}\n",
      "{'loss': 0.0036, 'learning_rate': 2.0000000000000003e-06, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768ce30b6e5b41df8686807928eb49d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08697407692670822, 'eval_accuracy': 0.9857996449911248, 'eval_precision': 0.9858045424494104, 'eval_recall': 0.9857996449911248, 'eval_runtime': 163.0616, 'eval_samples_per_second': 981.199, 'eval_steps_per_second': 3.833, 'epoch': 9.0}\n",
      "{'loss': 0.0028, 'learning_rate': 1.6000000000000001e-06, 'epoch': 9.2}\n",
      "{'loss': 0.003, 'learning_rate': 1.2000000000000002e-06, 'epoch': 9.4}\n",
      "{'loss': 0.0025, 'learning_rate': 8.000000000000001e-07, 'epoch': 9.6}\n",
      "{'loss': 0.0026, 'learning_rate': 4.0000000000000003e-07, 'epoch': 9.8}\n",
      "{'loss': 0.0028, 'learning_rate': 0.0, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a806456174ab4b929af026bc19b9e035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.08967098593711853, 'eval_accuracy': 0.9858308957723944, 'eval_precision': 0.9858352321445689, 'eval_recall': 0.9858308957723942, 'eval_runtime': 163.0296, 'eval_samples_per_second': 981.392, 'eval_steps_per_second': 3.834, 'epoch': 10.0}\n",
      "{'train_runtime': 15224.5774, 'train_samples_per_second': 420.364, 'train_steps_per_second': 1.642, 'train_loss': 0.022460230932235717, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63ca76784b0c49d2af3a7627f6ad6794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/625 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.08967098593711853, 'eval_accuracy': 0.9858308957723944, 'eval_precision': 0.9858352321445689, 'eval_recall': 0.9858308957723942, 'eval_runtime': 163.3287, 'eval_samples_per_second': 979.595, 'eval_steps_per_second': 3.827, 'epoch': 10.0}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# trainer.train(resume_from_checkpoint=r\"logs/OtherModels/bert_ag_results/last_epoch\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(r\"logs/OtherModels/distilbert_grambedding_results/last_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def calculate_metrics(bert_model):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    bert_model = bert_model.eval()\n",
    "    bert_model.to(device)\n",
    "    for _ in tqdm(test_dataset):\n",
    "        X = _['input_ids'].unsqueeze(0).to(device)\n",
    "        msk = _['attention_mask'].unsqueeze(0).to(device)\n",
    "        y = _['labels'].to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = bert_model(X, attention_mask=msk).logits\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    return y_pred, y_true\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    # y_pred = torch.tensor(y_pred)\n",
    "    y_true = torch.tensor(y_true)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    # y_true2 = torch.argmax(y_true, dim=1)\n",
    "    y_true2 = y_true\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1260,  6.5161]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model(test_dataset[0]['input_ids'].unsqueeze(0).to(device), attention_mask=test_dataset[0]['attention_mask'].unsqueeze(0).to(device)).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/159996 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159996/159996 [46:33<00:00, 57.28it/s] \n"
     ]
    }
   ],
   "source": [
    "y_pred, y_true = calculate_metrics(trainer.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.1260,  6.5161],\n",
       "        [-5.2008,  6.2141],\n",
       "        [ 6.8535, -7.7030],\n",
       "        ...,\n",
       "        [-5.5135,  6.7213],\n",
       "        [ 6.6492, -7.5064],\n",
       "        [-4.0169,  5.2082]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9873    0.9843    0.9858     79998\n",
      "           1     0.9844    0.9873    0.9859     79998\n",
      "\n",
      "    accuracy                         0.9858    159996\n",
      "   macro avg     0.9858    0.9858    0.9858    159996\n",
      "weighted avg     0.9858    0.9858    0.9858    159996\n",
      "\n",
      "confusion matrix:\n",
      " tensor([[78745,  1253],\n",
      "        [ 1014, 78984]])\n"
     ]
    }
   ],
   "source": [
    "cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "print(f'classification report: \\n {classification_report(y_true, torch.argmax(y_pred, dim=1), digits=4)}')\n",
    "print(f'confusion matrix:\\n {cm(y_pred, y_true)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([159996])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(y_pred, dim=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation results: {'eval_loss': 0.07060451805591583, 'eval_accuracy': 0.9905251842751843, 'eval_precision': 0.9877763381021575, 'eval_recall': 0.9839378848819438, 'eval_runtime': 300.514, 'eval_samples_per_second': 216.695, 'eval_steps_per_second': 8.668, 'epoch': 5.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9858533752195847"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.9877763381021575\n",
    "r = 0.9839378848819438\n",
    "(2*p*r)/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2654: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  66.96 M \n",
      "fwd MACs:                                                               5.51 GMACs\n",
      "fwd FLOPs:                                                              11.03 GFLOPS\n",
      "fwd+bwd MACs:                                                           16.54 GMACs\n",
      "fwd+bwd FLOPs:                                                          33.1 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "DistilBertForSequenceClassification(\n",
      "  66.96 M = 100% Params, 5.51 GMACs = 100% MACs, 11.03 GFLOPS = 100% FLOPs\n",
      "  (distilbert): DistilBertModel(\n",
      "    66.36 M = 99.11% Params, 5.51 GMACs = 99.98% MACs, 11.03 GFLOPS = 99.98% FLOPs\n",
      "    (embeddings): Embeddings(\n",
      "      23.84 M = 35.6% Params, 0 MACs = 0% MACs, 491.52 KFLOPS = 0% FLOPs\n",
      "      (word_embeddings): Embedding(23.44 M = 35.01% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(393.22 K = 0.59% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 512, 768)\n",
      "      (LayerNorm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 491.52 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      42.53 M = 63.51% Params, 5.51 GMACs = 99.98% MACs, 11.03 GFLOPS = 99.97% FLOPs\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          7.09 M = 10.59% Params, 918.55 MMACs = 16.66% MACs, 1.84 GFLOPS = 16.66% FLOPs\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            2.36 M = 3.53% Params, 314.57 MMACs = 5.71% MACs, 629.24 MFLOPS = 5.7% FLOPs\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "            (q_lin): Linear(590.59 K = 0.88% Params, 75.5 MMACs = 1.37% MACs, 150.99 MFLOPS = 1.37% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(590.59 K = 0.88% Params, 75.5 MMACs = 1.37% MACs, 150.99 MFLOPS = 1.37% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(590.59 K = 0.88% Params, 75.5 MMACs = 1.37% MACs, 150.99 MFLOPS = 1.37% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(590.59 K = 0.88% Params, 75.5 MMACs = 1.37% MACs, 150.99 MFLOPS = 1.37% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 491.52 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            4.72 M = 7.05% Params, 603.98 MMACs = 10.96% MACs, 1.21 GFLOPS = 10.95% FLOPs\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "            (lin1): Linear(2.36 M = 3.53% Params, 301.99 MMACs = 5.48% MACs, 603.98 MFLOPS = 5.47% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(2.36 M = 3.52% Params, 301.99 MMACs = 5.48% MACs, 603.98 MFLOPS = 5.47% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 491.52 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(590.59 K = 0.88% Params, 1.18 MMACs = 0.02% MACs, 2.36 MFLOPS = 0.02% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(3.08 K = 0% Params, 6.14 KMACs = 0% MACs, 12.29 KFLOPS = 0% FLOPs, in_features=768, out_features=4, bias=True)\n",
      "  (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('11.03 GFLOPS', '5.51 GMACs', '66.96 M')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_flops(model, input_shape=(2, 64), transformer_tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                                                    FLOP    % Total\n",
      "---------------------------------------------------  ---------  ---------\n",
      "DistilBertForSequenceClassification                  5512.501M    100.00%\n",
      " - aten.addmm                                        5437.004M     98.63%\n",
      " - aten.bmm                                            75.497M      1.37%\n",
      " DistilBertForSequenceClassification.distilbert      5511.315M     99.98%\n",
      "  - aten.addmm                                       5435.818M     98.61%\n",
      "  - aten.bmm                                           75.497M      1.37%\n",
      " DistilBertForSequenceClassification.pre_classifier     1.180M      0.02%\n",
      "  - aten.addmm                                          1.180M      0.02%\n",
      " DistilBertForSequenceClassification.classifier         0.006M      0.00%\n",
      "  - aten.addmm                                          0.006M      0.00%\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, max_length=64, truncation=True, padding='max_length', return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "flopt_counter = FlopCounterMode(model)\n",
    "with flopt_counter:\n",
    "    model(**encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 612351,
     "sourceId": 1095715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
