{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "dataset_dir = r'C:\\Users\\fardin\\Projects\\EnhanceSEO\\datasets\\MaliciousURL1\\malicious_phish.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# from IPython.display import clear_output\n",
    "# !pip install transformers datasets torch evaluate\n",
    "# clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu118'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from calflops import calculate_flops\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000])\n",
      "tensor(0.9958)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rand_nums = torch.randn(5000)\n",
    "print(rand_nums.shape)\n",
    "print(torch.std(rand_nums))\n",
    "plt.hist(rand_nums, bins=500)\n",
    "plt.hist(torch.fmod(rand_nums,2), bins=250)\n",
    "plt.hist(torch.fmod(rand_nums,2.)*(2./3.), bins=250)\n",
    "plt.xlim([-4, 4])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "from datasets import Dataset, load_metric\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_dir)\n",
    "df.columns=['text', 'Topic']\n",
    "df.dropna(inplace=True)\n",
    "class_list = df.Topic.unique()\n",
    "class_id = {t:i for i, t in enumerate(class_list)}\n",
    "id_class = {i:t for i, t in enumerate(class_list)}\n",
    "df['label'] = np.array([class_id[t] for t in df['Topic']], dtype=int)\n",
    "df = df.drop('Topic', axis=1)\n",
    "df_train, df_test = train_test_split(df, test_size=0.1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "test_dataset = Dataset.from_pandas(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DistilBertForSequenceClassification(\n",
       "  (distilbert): DistilBertModel(\n",
       "    (embeddings): Embeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (transformer): Transformer(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x TransformerBlock(\n",
       "          (attention): MultiHeadSelfAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (ffn): FFN(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (activation): GELUActivation()\n",
       "          )\n",
       "          (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e607dac78d7f47cc8e8b720c26f00638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/586071 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "292e5cdf671f41b78cdcb5bed176d669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/65120 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], truncation=True, max_length=64, padding='max_length')\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the format for PyTorch\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "test_dataset = test_dataset.rename_column('label', 'labels')\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=r'logs/OtherModels/bert_ag_results',\n",
    "    evaluation_strategy='epoch',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=25,\n",
    "    per_device_eval_batch_size=25,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    save_strategy=\"no\"\n",
    ")\n",
    "\n",
    "# Load the metrics\n",
    "import evaluate;\n",
    "accuracy_metric = evaluate.load('accuracy', trust_remote_code=True)\n",
    "precision_metric = evaluate.load('precision', trust_remote_code=True)\n",
    "recall_metric = evaluate.load('recall', trust_remote_code=True)\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p.predictions, p.label_ids\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    accuracy = accuracy_metric.compute(predictions=predictions, references=labels)\n",
    "    precision = precision_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    recall = recall_metric.compute(predictions=predictions, references=labels, average=\"macro\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy['accuracy'],\n",
    "        'precision': precision['precision'],\n",
    "        'recall': recall['recall']\n",
    "    }\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117215' max='117215' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117215/117215 10:55:26, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.049200</td>\n",
       "      <td>0.047129</td>\n",
       "      <td>0.987700</td>\n",
       "      <td>0.987093</td>\n",
       "      <td>0.977049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.030800</td>\n",
       "      <td>0.039084</td>\n",
       "      <td>0.990418</td>\n",
       "      <td>0.989069</td>\n",
       "      <td>0.982915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.023300</td>\n",
       "      <td>0.047980</td>\n",
       "      <td>0.990479</td>\n",
       "      <td>0.988586</td>\n",
       "      <td>0.983681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.011800</td>\n",
       "      <td>0.061964</td>\n",
       "      <td>0.990264</td>\n",
       "      <td>0.988507</td>\n",
       "      <td>0.982952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.070605</td>\n",
       "      <td>0.990525</td>\n",
       "      <td>0.987776</td>\n",
       "      <td>0.983938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2605' max='2605' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2605/2605 04:59]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 0.07060451805591583, 'eval_accuracy': 0.9905251842751843, 'eval_precision': 0.9877763381021575, 'eval_recall': 0.9839378848819438, 'eval_runtime': 300.514, 'eval_samples_per_second': 216.695, 'eval_steps_per_second': 8.668, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# trainer.train(resume_from_checkpoint=r\"logs/OtherModels/bert_ag_results/last_epoch\")\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(r\"logs/OtherModels/bert_ag_results/last_epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Evaluation results: {'eval_loss': 0.07060451805591583, 'eval_accuracy': 0.9905251842751843, 'eval_precision': 0.9877763381021575, 'eval_recall': 0.9839378848819438, 'eval_runtime': 300.514, 'eval_samples_per_second': 216.695, 'eval_steps_per_second': 8.668, 'epoch': 5.0}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9858533752195847"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.9877763381021575\n",
    "r = 0.9839378848819438\n",
    "(2*p*r)/(p+r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fardin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2654: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------------------------------- Calculate Flops Results -------------------------------------\n",
      "Notations:\n",
      "number of parameters (Params), number of multiply-accumulate operations(MACs),\n",
      "number of floating-point operations (FLOPs), floating-point operations per second (FLOPS),\n",
      "fwd FLOPs (model forward propagation FLOPs), bwd FLOPs (model backward propagation FLOPs),\n",
      "default model backpropagation takes 2.00 times as much computation as forward propagation.\n",
      "\n",
      "Total Training Params:                                                  66.96 M \n",
      "fwd MACs:                                                               5.51 GMACs\n",
      "fwd FLOPs:                                                              11.03 GFLOPS\n",
      "fwd+bwd MACs:                                                           16.54 GMACs\n",
      "fwd+bwd FLOPs:                                                          33.1 GFLOPS\n",
      "\n",
      "-------------------------------- Detailed Calculated FLOPs Results --------------------------------\n",
      "Each module caculated is listed after its name in the following order: \n",
      "params, percentage of total params, MACs, percentage of total MACs, FLOPS, percentage of total FLOPs\n",
      "\n",
      "Note: 1. A module can have torch.nn.module or torch.nn.functional to compute logits (e.g. CrossEntropyLoss). \n",
      " They are not counted as submodules in calflops and not to be printed out. However they make up the difference between a parent's MACs and the sum of its submodules'.\n",
      "2. Number of floating-point operations is a theoretical estimation, thus FLOPS computed using that could be larger than the maximum system throughput.\n",
      "\n",
      "DistilBertForSequenceClassification(\n",
      "  66.96 M = 100% Params, 5.51 GMACs = 100% MACs, 11.03 GFLOPS = 100% FLOPs\n",
      "  (distilbert): DistilBertModel(\n",
      "    66.36 M = 99.11% Params, 5.51 GMACs = 99.98% MACs, 11.03 GFLOPS = 99.98% FLOPs\n",
      "    (embeddings): Embeddings(\n",
      "      23.84 M = 35.6% Params, 0 MACs = 0% MACs, 491.52 KFLOPS = 0% FLOPs\n",
      "      (word_embeddings): Embedding(23.44 M = 35.01% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(393.22 K = 0.59% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, 512, 768)\n",
      "      (LayerNorm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 491.52 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "    )\n",
      "    (transformer): Transformer(\n",
      "      42.53 M = 63.51% Params, 5.51 GMACs = 99.98% MACs, 11.03 GFLOPS = 99.97% FLOPs\n",
      "      (layer): ModuleList(\n",
      "        (0-5): 6 x TransformerBlock(\n",
      "          7.09 M = 10.59% Params, 918.55 MMACs = 16.66% MACs, 1.84 GFLOPS = 16.66% FLOPs\n",
      "          (attention): MultiHeadSelfAttention(\n",
      "            2.36 M = 3.53% Params, 314.57 MMACs = 5.71% MACs, 629.24 MFLOPS = 5.7% FLOPs\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "            (q_lin): Linear(590.59 K = 0.88% Params, 75.5 MMACs = 1.37% MACs, 150.99 MFLOPS = 1.37% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (k_lin): Linear(590.59 K = 0.88% Params, 75.5 MMACs = 1.37% MACs, 150.99 MFLOPS = 1.37% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (v_lin): Linear(590.59 K = 0.88% Params, 75.5 MMACs = 1.37% MACs, 150.99 MFLOPS = 1.37% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "            (out_lin): Linear(590.59 K = 0.88% Params, 75.5 MMACs = 1.37% MACs, 150.99 MFLOPS = 1.37% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (sa_layer_norm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 491.52 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "          (ffn): FFN(\n",
      "            4.72 M = 7.05% Params, 603.98 MMACs = 10.96% MACs, 1.21 GFLOPS = 10.95% FLOPs\n",
      "            (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.1, inplace=False)\n",
      "            (lin1): Linear(2.36 M = 3.53% Params, 301.99 MMACs = 5.48% MACs, 603.98 MFLOPS = 5.47% FLOPs, in_features=768, out_features=3072, bias=True)\n",
      "            (lin2): Linear(2.36 M = 3.52% Params, 301.99 MMACs = 5.48% MACs, 603.98 MFLOPS = 5.47% FLOPs, in_features=3072, out_features=768, bias=True)\n",
      "            (activation): GELUActivation(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs)\n",
      "          )\n",
      "          (output_layer_norm): LayerNorm(1.54 K = 0% Params, 0 MACs = 0% MACs, 491.52 KFLOPS = 0% FLOPs, (768,), eps=1e-12, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (pre_classifier): Linear(590.59 K = 0.88% Params, 1.18 MMACs = 0.02% MACs, 2.36 MFLOPS = 0.02% FLOPs, in_features=768, out_features=768, bias=True)\n",
      "  (classifier): Linear(3.08 K = 0% Params, 6.14 KMACs = 0% MACs, 12.29 KFLOPS = 0% FLOPs, in_features=768, out_features=4, bias=True)\n",
      "  (dropout): Dropout(0 = 0% Params, 0 MACs = 0% MACs, 0 FLOPS = 0% FLOPs, p=0.2, inplace=False)\n",
      ")\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('11.03 GFLOPS', '5.51 GMACs', '66.96 M')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_flops(model, input_shape=(2, 64), transformer_tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                                                    FLOP    % Total\n",
      "---------------------------------------------------  ---------  ---------\n",
      "DistilBertForSequenceClassification                  5512.501M    100.00%\n",
      " - aten.addmm                                        5437.004M     98.63%\n",
      " - aten.bmm                                            75.497M      1.37%\n",
      " DistilBertForSequenceClassification.distilbert      5511.315M     99.98%\n",
      "  - aten.addmm                                       5435.818M     98.61%\n",
      "  - aten.bmm                                           75.497M      1.37%\n",
      " DistilBertForSequenceClassification.pre_classifier     1.180M      0.02%\n",
      "  - aten.addmm                                          1.180M      0.02%\n",
      " DistilBertForSequenceClassification.classifier         0.006M      0.00%\n",
      "  - aten.addmm                                          0.006M      0.00%\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "text = \"Replace me by any text you'd like.\"\n",
    "encoded_input = tokenizer(text, max_length=64, truncation=True, padding='max_length', return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "flopt_counter = FlopCounterMode(model)\n",
    "with flopt_counter:\n",
    "    model(**encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 612351,
     "sourceId": 1095715,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30699,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
