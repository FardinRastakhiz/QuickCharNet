{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "dataset_dir = r'C:\\Users\\fardin\\Projects\\EnhanceSEO\\datasets\\MaliciousURL1\\malicious_phish.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import zlib\n",
    "# file_name = '600_650_batch_ugxHSbe5cpWquoeKBxEcEPaz'\n",
    "# with open(rf'TestsOnMaliciousURL1\\ChatGPTData\\{file_name}', 'rb') as f:\n",
    "#     decompressed = zlib.decompress(f.read()).decode()\n",
    "\n",
    "# with open(rf'TestsOnMaliciousURL1\\ChatGPTData\\{file_name}.jsonl', 'wt') as f:\n",
    "#     f.write(decompressed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "from copy import copy, deepcopy\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch_scatter import scatter_max\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_scatter import scatter_sum\n",
    "from torch_scatter import scatter_std\n",
    "import torchmetrics\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.flop_counter import FlopCounterMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1024\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_dir)\n",
    "df.columns=['Address', 'Topic']\n",
    "df.dropna(inplace=True)\n",
    "class_list = df.Topic.unique()\n",
    "class_id = {t:i for i, t in enumerate(class_list)}\n",
    "id_class = {i:t for i, t in enumerate(class_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_dict = {\"custom_id\": \"\", \"method\": \"POST\", \"url\": \"/v1/embeddings\", \"body\": {\"model\": \"text-embedding-3-small\", \"input\": \"\",\"max_tokens\": 1000}}\n",
    "intervals = 50000\n",
    "previous = 0\n",
    "for i in range(intervals, len(df)+intervals, intervals):\n",
    "    i = min(len(df), i)\n",
    "    print(previous, i)\n",
    "    list_of_urls = []\n",
    "    for j in range(previous, i):\n",
    "        url = df['url'].iloc[j]\n",
    "        template_dict[\"custom_id\"] = f\"url-{j}\"\n",
    "        template_dict[\"body\"][\"input\"] = F\"Cl{url}\"\n",
    "        list_of_urls.append(json.dumps(template_dict))\n",
    "        \n",
    "    with open(fr\"TestsOnMaliciousURL1\\ChatGPTData\\samples_{previous}_{i}.jsonl\", 'ta') as f:\n",
    "        for url in list_of_urls:\n",
    "            f.write(url + \"\\n\")\n",
    "    previous = i\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPTEmbeddingPaths = [\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\0_50_batch_uDZTTt9s8Cjyag0CVRuZxPtn_output.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\50_100_batch_1iT04A6jdjUhSyrjJmk8hUEw_output.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\100_150_batch_Lfrz2pFzhUPGmv5cItIJk1q1_output.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\150_200_batch_QIB4ydKaFwYUTGv5R7K1C4fu_output.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\200_250_batch_Nsc3rpUsTBVbFa8CaB5UWPXw.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\250_300_batch_3ZTyxxMrukZomJfRlempxB0Z.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\300_350_batch_CDGbePCcdBiqK5RsW7czOK6O.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\350_400_batch_zk5FnhCB5JAxu4fupZTQkxbk.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\400_450_batch_Q2o70ATqnH2yRAg4SWD8JXqk.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\450_500_batch_4YySMvPDyyCkUjViQIafcuzd.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\500_550_batch_CU0mEgcbOGt4ylwNhWDRNbq9.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\550_600_batch_Jm9Pdo74WpPk4tvsTpQnDmz2.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\600_650_batch_ugxHSbe5cpWquoeKBxEcEPaz.jsonl',\n",
    "    r'TestsOnMaliciousURL1\\ChatGPTData\\650_651_batch_0eIufBJHsSd2vMEAJ97RYI6u_output.jsonl'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_embeddings = {}\n",
    "for emb_path in GPTEmbeddingPaths:\n",
    "    with open(emb_path, 'rt') as f:\n",
    "        all_lines = []\n",
    "        for line in f.readlines():\n",
    "            all_lines.append(json.loads(line))\n",
    "        for url_data in all_lines:\n",
    "            data_embeddings[int(url_data['custom_id'][4:])] = np.array(url_data['response']['body']['data'][0]['embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = len(url_data['response']['body']['data'][0]['embedding'])\n",
    "embeddings = torch.zeros((len(data_embeddings), embedding_dim))\n",
    "labels = torch.zeros((len(data_embeddings), len(class_id)))\n",
    "for i, (k, v) in enumerate(data_embeddings.items()):\n",
    "    embeddings[i] = torch.from_numpy(v)\n",
    "    labels[i][class_id[df['Topic'].iloc[k]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embs, test_embs, train_labels, test_labels = train_test_split(embeddings, labels, test_size=0.1, shuffle=True)\n",
    "train_dataset = TensorDataset(train_embs, train_labels)\n",
    "test_dataset = TensorDataset(test_embs, test_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTEmbeddingClassifier(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, num_classes, *args, **kwargs) -> None:\n",
    "        super(GPTEmbeddingClassifier, self).__init__(*args, **kwargs)\n",
    "        self.fc1 = nn.Linear(input_dim, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(ClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(\"model\", logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, *args, **kwargs):\n",
    "        return self.model(x)\n",
    "\n",
    "    # def on_train_epoch_start(self) -> None:\n",
    "    #     param_groups = next(iter(self.optimizer.param_groups))\n",
    "    #     if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "    #         current_learning_rate = float(param_groups[\"lr\"])\n",
    "    #         self.log(\n",
    "    #             \"lr\",\n",
    "    #             current_learning_rate,\n",
    "    #             batch_size=self.batch_size,\n",
    "    #             on_epoch=True,\n",
    "    #             on_step=False,\n",
    "    #         )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True\n",
    "        )\n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.00339\n",
    "input_dim = 1536 \n",
    "num_classes = len(class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# for i in range(5):\n",
    "callbacks = [\n",
    "    # EarlyStopping(monitor='val_loss',mode='min',patience=25),\n",
    "    # CustomModelCheckpoint(dirpath=r'models\\malicious_urls_model', filename=f'malicious_urls_model_', every_n_epochs=3, mode='min', monitor='val_loss_epoch', save_on_train_epoch_end=True),\n",
    "    ModelCheckpoint(save_top_k=5, mode='min', monitor='val_loss', save_last=True)\n",
    "    ]\n",
    "\n",
    "classifier_torch_model = GPTEmbeddingClassifier(embedding_dim, num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=0.00001)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50, 75, 100, 125, 150, 175],gamma=0.5)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "classfier_lightning_model = ClassifierLightningModel(classifier_torch_model, \n",
    "                                                    num_classes=num_classes,\n",
    "                                            learning_rate=lr,\n",
    "                                            batch_size=batch_size,\n",
    "                                            optimizer=optimizer,\n",
    "                                            loss_func=loss_func,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            user_lr_scheduler=False\n",
    "                                            ).to(device)\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import lightning as L\n",
    "\n",
    "trainer = L.Trainer(\n",
    "            callbacks=callbacks,\n",
    "            max_epochs=200,\n",
    "            accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            logger=CSVLogger(save_dir='logs/', name='log2')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from lightning.pytorch.tuner import Tuner\n",
    "# tuner = Tuner(trainer)\n",
    "# result = tuner.lr_find(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader, min_lr=0.000001, max_lr=0.1, num_training=2000)\n",
    "# fig = result.plot(suggest=True)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Module                            FLOP    % Total\n",
      "---------------------------  ---------  ---------\n",
      "GPTEmbeddingClassifier       1881.145M    100.00%\n",
      " - aten.addmm                1881.145M    100.00%\n",
      " GPTEmbeddingClassifier.fc1  1610.613M     85.62%\n",
      "  - aten.addmm               1610.613M     85.62%\n",
      " GPTEmbeddingClassifier.fc2   268.435M     14.27%\n",
      "  - aten.addmm                268.435M     14.27%\n",
      " GPTEmbeddingClassifier.fc3     2.097M      0.11%\n",
      "  - aten.addmm                  2.097M      0.11%\n"
     ]
    }
   ],
   "source": [
    "X, y = next(iter(test_dataloader))\n",
    "flopt_counter = FlopCounterMode(classfier_lightning_model.model)\n",
    "with flopt_counter:\n",
    "    classfier_lightning_model.model(X.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                   | Params\n",
      "-----------------------------------------------------\n",
      "0 | model     | GPTEmbeddingClassifier | 919 K \n",
      "1 | loss_func | BCEWithLogitsLoss      | 0     \n",
      "2 | train_acc | MulticlassAccuracy     | 0     \n",
      "3 | val_acc   | MulticlassAccuracy     | 0     \n",
      "4 | test_acc  | MulticlassAccuracy     | 0     \n",
      "-----------------------------------------------------\n",
      "919 K     Trainable params\n",
      "0         Non-trainable params\n",
      "919 K     Total params\n",
      "3.677     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8875adfe54e422280f29250765b548b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8173ba9f644edb92a4484f9ad5588e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77cf0ece245542f09283b98b6d4c6f45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac58509d2cc84867bb84ec17be314339",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fd8b45bb9f846609d4c4202041f744e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6421eb96884944d0af3feab7544a6f79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9f7422f7454845af77b0b315cfbe15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121dea20d3c644438698f8823d60e81a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f481978107c2434b84c02e60fd297647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9a55ac658c245ee83d40c3ba17ba2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465585c2b7474f648f39e44f1b214f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c8d4e0114ea4112814fc76e932e063b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0638829b1758487685312a6e31805403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "def calculate_metrics(cl_model):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y in tqdm(test_dataloader):\n",
    "        X = X.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 33.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9482    0.9266    0.9373       395\n",
      "           1     0.9903    0.9962    0.9932      3672\n",
      "           2     0.9841    0.9852    0.9846       944\n",
      "           3     0.9158    0.7982    0.8529       109\n",
      "\n",
      "    accuracy                         0.9846      5120\n",
      "   macro avg     0.9596    0.9265    0.9420      5120\n",
      "weighted avg     0.9843    0.9846    0.9843      5120\n",
      "\n",
      "confusion matrix:\n",
      " tensor([[ 366,   19,    7,    3],\n",
      "        [   9, 3658,    3,    2],\n",
      "        [   0,   11,  930,    3],\n",
      "        [  11,    6,    5,   87]])\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "classfier_lightning_model = classfier_lightning_model.eval()\n",
    "calculate_metrics(classfier_lightning_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
