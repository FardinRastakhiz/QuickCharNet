{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "dataset_dir = r'C:\\Users\\fardin\\Projects\\EnhanceSEO\\datasets\\MaliciousURL1\\malicious_phish.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cu118'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer\n",
    "from urllib.parse import urlparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.tokenize import word_tokenize, TweetTokenizer\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from torch_scatter import scatter_max\n",
    "from torch_scatter import scatter_mean\n",
    "from torch_scatter import scatter_sum\n",
    "from torch_scatter import scatter_std\n",
    "import torchmetrics\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from torch.utils.flop_counter import FlopCounterMode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank\"\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 1024\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>br-icloud.com.br</td>\n",
       "      <td>phishing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mp3raid.com/music/krizz_kaliko.html</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bopsecrets.org/rexroth/cr/1.htm</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.garage-pirenne.be/index.php?option=...</td>\n",
       "      <td>defacement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://adventure-nicaragua.net/index.php?optio...</td>\n",
       "      <td>defacement</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url        type\n",
       "0                                   br-icloud.com.br    phishing\n",
       "1                mp3raid.com/music/krizz_kaliko.html      benign\n",
       "2                    bopsecrets.org/rexroth/cr/1.htm      benign\n",
       "3  http://www.garage-pirenne.be/index.php?option=...  defacement\n",
       "4  http://adventure-nicaragua.net/index.php?optio...  defacement"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(dataset_dir).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(dataset_dir)\n",
    "df.columns=['Address', 'Topic']\n",
    "df.dropna(inplace=True)\n",
    "class_list = df.Topic.unique()\n",
    "class_id = {t:i for i, t in enumerate(class_list)}\n",
    "id_class = {i:t for i, t in enumerate(class_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'phishing': 0, 'benign': 1, 'defacement': 2, 'malware': 3}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = set([c for a in df.Address.values for c in ' '.join(bert_tokenizer.tokenize(a))])\n",
    "vocab_dict = {c:i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterandTokenLevelCustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, X, y, num_classes, dictionary, tokenizer, doc_token_length=256, doc_char_length=1024) -> None:\n",
    "        super().__init__()\n",
    "        self.doc_char_length = doc_char_length\n",
    "        self.doc_teken_length = doc_token_length\n",
    "        \n",
    "        \n",
    "        y = torch.from_numpy(np.array([class_id[c] for c in y], dtype=np.longlong))\n",
    "        self.y = torch.nn.functional.one_hot(y, num_classes=num_classes).float()\n",
    "        \n",
    "        self.dictionary = dictionary\n",
    "        self.dict_keys = list(dictionary.keys())\n",
    "        self.vocab_size = len(self.dictionary)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        self.X = torch.zeros((len(X), self.doc_char_length), dtype=torch.long)\n",
    "        # self.token_lengths = torch.zeros((len(X), self.doc_teken_length), dtype=torch.int)\n",
    "        self.token_indices = torch.zeros((len(X), self.doc_char_length), dtype=torch.long)\n",
    "        \n",
    "        for i, doc in enumerate(X):\n",
    "            tokens = self.tokenizer(doc)\n",
    "            tokens.append(self.dict_keys[-1])\n",
    "            doc = ' '.join(tokens)\n",
    "            \n",
    "            indices = torch.from_numpy(np.array([self.dictionary[t] for t in doc if t in self.dictionary], dtype=np.longlong))\n",
    "            char_pad_size = max(self.doc_char_length - len(indices), 0)\n",
    "            \n",
    "            self.X[i] = torch.nn.functional.pad(indices[:self.doc_char_length], (0,char_pad_size))\n",
    "            \n",
    "            token_lengths = torch.from_numpy(np.array([len(t) for t in tokens], dtype=np.longlong))+1\n",
    "            token_lengths[-1] -= 1\n",
    "            token_indices = torch.repeat_interleave(torch.arange(len(token_lengths), dtype=torch.long), token_lengths)\n",
    "            token_indices = torch.nn.functional.pad(token_indices[:self.doc_char_length], (0,char_pad_size), value=len(token_lengths)-1)\n",
    "            self.token_indices[i] = token_indices\n",
    "        \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index], self.token_indices[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_for_Text(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_embedding, batch_size, max_char_count, embedding_dim=64, hidden_dim=64, dropout=0.3, num_out_features=4, kernel_size=[5, 5, 3, 3], seed=-1, *args, **kwargs) -> None:\n",
    "        super(CNN_for_Text, self).__init__(*args, **kwargs)\n",
    "        self.batch_size = batch_size\n",
    "        self.max_char_count = max_char_count\n",
    "        if seed>-1:\n",
    "            torch.manual_seed(seed)\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embedding, embedding_dim)\n",
    "        # nn.init.normal_(self.embedding.weight)\n",
    "        self.conv1 = nn.Conv1d(embedding_dim, hidden_dim, kernel_size=kernel_size[0], padding=kernel_size[0]//2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=kernel_size[1], padding=kernel_size[1]//2)\n",
    "        \n",
    "        # self.conv3 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(2*hidden_dim, hidden_dim, kernel_size=kernel_size[2], padding=kernel_size[2]//2)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv4 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=kernel_size[3], padding=kernel_size[3]//2)\n",
    "        \n",
    "        self.globalpool = nn.AdaptiveMaxPool1d(hidden_dim)\n",
    "        self.fc1 = nn.Linear(hidden_dim * hidden_dim, 32)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc_out = nn.Linear(32, num_out_features)\n",
    "    \n",
    "    def forward(self, x, token_counts):\n",
    "        x = self.embedding(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        # x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x1 = scatter_max(x,token_counts, dim=1)[0]\n",
    "        x2 = scatter_mean(x,token_counts, dim=1)\n",
    "        # x2 = scatter_sum(x,token_counts, dim=1)\n",
    "        # x2 = scatter_std(x,token_counts, dim=1)\n",
    "        x = torch.cat([x1, x2], dim=2)\n",
    "        # x = F.layer_norm(x, (x.shape[1], x.shape[2]))\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.globalpool(x)\n",
    "        x = F.elu_(self.fc1(x.view(x.shape[0], -1)))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ClassifierLightningModel(L.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        num_classes,\n",
    "        optimizer=None,\n",
    "        loss_func=None,\n",
    "        learning_rate=0.01,\n",
    "        batch_size=64,\n",
    "        lr_scheduler=None,\n",
    "        user_lr_scheduler=False,\n",
    "        min_lr=0.0,\n",
    "    ):\n",
    "        super(ClassifierLightningModel, self).__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.min_lr = min_lr\n",
    "        # self.save_hyperparameters(ignore=[\"model\"])\n",
    "        self.save_hyperparameters(\"model\", logger=False)\n",
    "        self.optimizer = self._get_optimizer(optimizer)\n",
    "        self.lr_scheduler = (\n",
    "            self._get_lr_scheduler(lr_scheduler) if user_lr_scheduler else None\n",
    "        )\n",
    "        self.loss_func = loss_func\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=num_classes)\n",
    "\n",
    "    def forward(self, x, token_count, *args, **kwargs):\n",
    "        return self.model(x, token_count)\n",
    "\n",
    "    # def on_train_epoch_start(self) -> None:\n",
    "    #     param_groups = next(iter(self.optimizer.param_groups))\n",
    "    #     if \"lr\" in param_groups and param_groups[\"lr\"] is not None:\n",
    "    #         current_learning_rate = float(param_groups[\"lr\"])\n",
    "    #         self.log(\n",
    "    #             \"lr\",\n",
    "    #             current_learning_rate,\n",
    "    #             batch_size=self.batch_size,\n",
    "    #             on_epoch=True,\n",
    "    #             on_step=False,\n",
    "    #         )\n",
    "\n",
    "    def training_step(self, batch, *args, **kwargs):\n",
    "        X, y, token_count = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.train()\n",
    "        y_out = self(X, token_count)\n",
    "\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.train_losses.append(loss.detach().item())\n",
    "        self.log(\n",
    "            \"train_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True,\n",
    "            on_step=True,\n",
    "        )\n",
    "        \n",
    "        self.train_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('train_acc', self.train_acc, prog_bar=True, on_epoch=True, on_step=True, batch_size=self.batch_size)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, *args, **kwargs):\n",
    "        X, y, token_count = batch\n",
    "        X.to(self.device)\n",
    "        y.to(self.device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        y_out = self(X, token_count)\n",
    "        loss = self.loss_func(y_out.view(y.shape), y )\n",
    "        self.val_losses.append(loss.detach().item())\n",
    "\n",
    "        self.log(\n",
    "            \"val_loss\",\n",
    "            loss,\n",
    "            prog_bar=True,\n",
    "            batch_size=self.batch_size,\n",
    "            on_epoch=True\n",
    "        )\n",
    "        \n",
    "        self.val_acc(torch.argmax(y_out, dim=1), torch.argmax(y, dim=1))\n",
    "        self.log('val_acc', self.val_acc, prog_bar=True, on_epoch=True, batch_size=self.batch_size)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.lr_scheduler is None:\n",
    "            return self.optimizer\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": self.lr_scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def update_learning_rate(self, learning_rate: float):\n",
    "        self.learning_rate = learning_rate\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = learning_rate\n",
    "\n",
    "    def _get_optimizer(self, optimizer):\n",
    "        return (\n",
    "            optimizer\n",
    "            if optimizer is not None\n",
    "            else torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        )\n",
    "\n",
    "    def _get_lr_scheduler(self, lr_scheduler):\n",
    "        return (\n",
    "            lr_scheduler\n",
    "            if lr_scheduler is not None\n",
    "            else torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                self.optimizer, patience=5, factor=0.5, mode=\"min\", min_lr=self.min_lr\n",
    "            )\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr= 0.0036\n",
    "output_size = 128\n",
    "hidden_dim = 64\n",
    "embedding_dim = 64\n",
    "label_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(df, test_size=0.1, shuffle=True)\n",
    "train_dataset = CharacterandTokenLevelCustomDataset(df_train.Address.values, df_train.Topic.values, len(class_id), vocab_dict, bert_tokenizer.tokenize, doc_token_length=64, doc_char_length=256)\n",
    "test_dataset = CharacterandTokenLevelCustomDataset(df_test.Address.values, df_test.Topic.values, len(class_id), vocab_dict, bert_tokenizer.tokenize, doc_token_length=64, doc_char_length=256)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, drop_last=True, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "seed = 911\n",
    "# for i in range(5):\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss',mode='min',patience=25),\n",
    "    # CustomModelCheckpoint(dirpath=r'models\\malicious_urls_model', filename=f'malicious_urls_model_', every_n_epochs=3, mode='min', monitor='val_loss_epoch', save_on_train_epoch_end=True),\n",
    "    ModelCheckpoint(save_top_k=5, mode='min', monitor='val_loss', save_last=True)\n",
    "    ]\n",
    "\n",
    "classifier_torch_model = CNN_for_Text(num_embedding=len(vocab_dict), batch_size=batch_size, hidden_dim=hidden_dim, embedding_dim=embedding_dim, max_char_count=256, dropout=0.15, num_out_features=len(class_id), kernel_size=[5, 5, 5, 3], seed=seed).to(device)\n",
    "optimizer = torch.optim.Adam(classifier_torch_model.parameters(), lr=lr, weight_decay=0.00011)\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[25, 50, 75, 100, 125, 150, 175],gamma=0.5)\n",
    "loss_func = torch.nn.BCEWithLogitsLoss()\n",
    "classfier_lightning_model = ClassifierLightningModel(classifier_torch_model, \n",
    "                                                    num_classes=len(class_id),\n",
    "                                            learning_rate=lr,\n",
    "                                            batch_size=batch_size,\n",
    "                                            optimizer=optimizer,\n",
    "                                            loss_func=loss_func,\n",
    "                                            lr_scheduler=lr_scheduler,\n",
    "                                            user_lr_scheduler=True\n",
    "                                            ).to(device)\n",
    "\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "import lightning as L\n",
    "\n",
    "trainer = L.Trainer(\n",
    "            callbacks=callbacks,\n",
    "            max_epochs=200,\n",
    "            accelerator= 'gpu' if torch.cuda.is_available() else 'cpu',\n",
    "            logger=CSVLogger(save_dir='logs/', name='log2')\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, tc = next(iter(test_dataloader))\n",
    "flopt_counter = FlopCounterMode(classfier_lightning_model.model)\n",
    "with flopt_counter:\n",
    "    classfier_lightning_model.model(X.to(device), tc.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 205\n",
    "trainer.fit_loop.max_epochs = max_epochs\n",
    "trainer.fit(classfier_lightning_model, train_dataloaders=train_dataloader, val_dataloaders=test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "from torchmetrics.classification import ConfusionMatrix\n",
    "def calculate_metrics(cl_model):\n",
    "    cm = ConfusionMatrix(task=\"multiclass\", num_classes=len(class_id))\n",
    "\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "\n",
    "    cl_model = cl_model.eval()\n",
    "    cl_model.to(device)\n",
    "    for X, y, tc in tqdm(test_dataloader):\n",
    "        X = X.to(device)\n",
    "        tc = tc.to(device)\n",
    "        with torch.no_grad():\n",
    "            y_p = cl_model(X, tc)\n",
    "            y_p = y_p.cpu()\n",
    "        y_pred.append(y_p)\n",
    "        y_true.append(y)\n",
    "    y_pred = torch.cat(y_pred, dim=0)\n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred2 = torch.argmax(y_pred, dim=1)\n",
    "    y_true2 = torch.argmax(y_true, dim=1)\n",
    "    print(f'classification report: \\n {classification_report(y_true2, y_pred2, digits=4)}')\n",
    "    print(f'confusion matrix:\\n {cm(y_pred2, y_true2)}')\n",
    "    print('================================')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [00:02<00:00, 22.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9536    0.9522    0.9529      9327\n",
      "           1     0.9910    0.9924    0.9917     42281\n",
      "           2     0.9944    0.9991    0.9967      9606\n",
      "           3     0.9919    0.9654    0.9785      3298\n",
      "\n",
      "    accuracy                         0.9862     64512\n",
      "   macro avg     0.9827    0.9773    0.9800     64512\n",
      "weighted avg     0.9862    0.9862    0.9862     64512\n",
      "\n",
      "confusion matrix:\n",
      " tensor([[ 8881,   375,    49,    22],\n",
      "        [  317, 41959,     1,     4],\n",
      "        [    9,     0,  9597,     0],\n",
      "        [  106,     4,     4,  3184]])\n",
      "================================\n"
     ]
    }
   ],
   "source": [
    "classfier_lightning_model.model = classfier_lightning_model.model.eval()\n",
    "classfier_lightning_model = classfier_lightning_model.eval()\n",
    "calculate_metrics(classfier_lightning_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9799925612244897"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = 0.9827\n",
    "r = 0.9773\n",
    "(2*p*r)/(p+r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
